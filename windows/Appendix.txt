
 I write some updating points in Parity Volume Set Specification 3.0.
This is a reminder for later implementation.



##### Start packet

 It's possible to remove the first field (a globally unique random number).
The unique random number is set to randomize InputSetID internally.
No need to store the number in Start Packet, because it's not refered later.

-------------------------------------------------------------------------------
Old construction was below.

*Table: Start Packet Body Contents*

| Length (bytes) | Type | Description |
|---------------:|:-----|:------------|
|  8 | Random | A globally unique random number |
|  8 | InputSetID | The "parent" InputSetID, or zeros if none. |
| 16 | fingerprint hash | The checksum of the parent's Root packet, or zeros if none. |
|  8 | unsigned int | Block size in bytes |
|  1 | unsigned int | The size of the Galois field in bytes. |
|  ? | ?-byte GF | The generator of the Galois field without its leading 1. |

-------------------------------------------------------------------------------
New construction is below.

*Table: Start Packet Body Contents*

| Length (bytes) | Type | Description |
|---------------:|:-----|:------------|
|  8 | InputSetID | The "parent" InputSetID, or zeros if none. |
| 16 | fingerprint hash | The checksum of the parent's Root packet, or zeros if none. |
|  8 | unsigned int | Block size in bytes |
|  1 | unsigned int | The size of the Galois field in bytes. |
|  ? | ?-byte GF | The generator of the Galois field without its leading 1. |

-------------------------------------------------------------------------------



##### External Data Packet

 External Data Packet should exclude input blocks of chunk tails.
Because File Packet includes fingerprint hash of every chunk tails,
External Data Packet doesn't need to contain checksums of blocks for chunk tails.
When a PAR3 client omits checksums of such input blocks,
it makes multiple External Data Packets to contain full size blocks only.

 Even when a PAR3 client stores checksums of all input blocks,
other PAR3 clients may ignore blocks of chunk tails.
It's difficult to verify checksums of blocks for chunk tails.

 When an input block contains a chunk tail, it may store the tail data in any position.
(Normally, it would put chunk tail with offset = 0.)
To calculate rolling checksum and fingerprint hash for this kind of blocks,
the block's non-used area is filled by zeros.

 For example, a chunk tail occupies 20 KB in an input block, while block size is 100 KB.
It calculates checksum and hash values for the whole 100 KB block data,
which consists of 20 KB tail data and 80 KB zeros.
The resulting checksum differs by the tail data's offset.

[  example of full block data   ]
[ tail data ] [      zeros      ] : It puts chunk tail before zeros.
[      zeros      ] [ tail data ] : It puts chunk tail after zeros.
[ zeros ] [ tail data ] [ zeros ] : It puts chunk tail between zeros.

 When an input block contains multiple chunk tails,
it will be difficult to use their checksums at verification time.
The layout of chunk tails is unpredictable.
Some chunk tails may overlap each other.
There may be space between chunk tails.
Thus, External Data Packet isn't good to include such input blocks,
which contain multiple chunk tails.

 But, if External Data Packet includes this kind of blocks,
the block's non-used area is filled by zeros.
To check the checksum of the input block,
it needs to put all chunk tails on their position,
and it calculates checksum of the whole data.
It requires temporary buffer to keep the block data.

[    example of full block data     ]
[ tail data ] [ tail data ] [ zeros ] : It puts chunk tails before zeros.
[ zeros ] [ tail data ] [ tail data ] : It puts chunk tails after zeros.
[ tail data ] [ zeros ] [ tail data ] : It puts chunk tails before and after zeros.
[ tail data  [ overlap ]  tail data ] : It puts chunk tails by overlaping them.



#### File Packet

 It will be possible to use other fingerprint hashes as options in future.
(For example, I may make a new packet type, Hash Packet.)
If it doesn't use BLAKE3 for the input file, the fourth field is all zeroes.
Even when this packet doesn't contain BLAKE3 hash for the protected data in the file,
it may verify files by using checksums for input blocks in External Data Packet.

 When "Par inside" feature is used, some of the file's data may not be protected.
Because this feature may modify file data to insert PAR3 packets inside the original file,
it must calculate hash values for the original state of file data.
They are "rolling hash of the first 16kB of the original file"
and "fingerprint hash of the original data in the file".

 After inserting unprotected chunks,
these hash values may become useless to detect damage of protected chunks.
But, it won't be a problem.
Because "Par inside" feature only protects a file itself,
it won't use "rolling hash" to find another misnamed file.
Because "Par inside" feature includes unprotected chunks always,
it cannot use "fingerprint hash" to detect damage of the file anyway.

 Even when these hash values are useless after adapting "Par inside",
it's important to remove the inserted PAR3 packets from outside file.
It's possible to restore original form of file.
It can use hash values to confirm that the original data is correct.

-------------------------------------------------------------------------------
Old construction was below.

*Table: File Packet Body Contents*

| Length (bytes) | Type | Description |
|---------------:|:-----|:------------|
|  2 | unsigned int | length of filename in bytes |
|  ? | UTF-8 string | filename |
|  8 | rolling hash | hash of the first 16kB of the file |
| 16 | fingerprint hash | hash of the protected data in the file |
|  1 | unsigned int | number of options (a.k.a. permissions) |
| 16*? | fingerprint hashes | checksums of packets for options |
| ?*?  | chunk descriptions |  see below. |

-------------------------------------------------------------------------------
New construction is below.

*Table: File Packet Body Contents*

| Length (bytes) | Type | Description |
|---------------:|:-----|:------------|
|  2 | unsigned int | length of filename in bytes |
|  ? | UTF-8 string | filename |
|  8 | rolling hash | hash of the first 16kB of the original file |
| 16 | fingerprint hash | hash of the original data in the file, or zeros if not used |
|  1 | unsigned int | number of options (a.k.a. permissions) |
| 16*? | fingerprint hashes | checksums of packets for options |
| ?*?  | chunk descriptions |  see below. |

-------------------------------------------------------------------------------



##### UNIX Permissions Packet

 I changed the default value for i_mode.
By doing this change, all non used values are same 0xff bytes.
It's easy to see which value is set or not.

-------------------------------------------------------------------------------
Old construction was below.

*Table: UNIX Permissions Packet Unset Values

| Field | Unset Value |
|------:|------------:|
| atime, ctime, mtime | 2^64-1 (maximum unsigned value) |
| UID, GID | 2^32-1 (maximum unsigned value) |
| i_mode | 0x0180 (owner read+write) |
| owner name, group name | empty string | 
| xattr | empty list |

-------------------------------------------------------------------------------
New construction is below.

*Table: UNIX Permissions Packet Unset Values

| Field | Unset Value |
|------:|------------:|
| atime, ctime, mtime | 2^64-1 (maximum unsigned value) |
| UID, GID | 2^32-1 (maximum unsigned value) |
| i_mode | 0xffff | value to indicate unset
| owner name, group name | empty string | 
| xattr | empty list |

-------------------------------------------------------------------------------



##### FAT Permissions Packet

 I changed the default value for FileAttributes.
By doing this change, all non used values are same 0xff bytes.
It's easy to see which value is set or not.

-------------------------------------------------------------------------------
Old construction was below.

*Table: FAT Permissions Packet Unset Values

| Field | Unset Value |
|------:|------------:|
| CreationTimestamp, LastAccessTimestamp, LastWriteTimestamp |  2^64-1 (maximum unsigned value)  |
| FileAttributes | 0 | 

-------------------------------------------------------------------------------
New construction is below.

*Table: FAT Permissions Packet Unset Values

| Field | Unset Value |
|------:|------------:|
| CreationTimestamp, LastAccessTimestamp, LastWriteTimestamp |  2^64-1 (maximum unsigned value)  |
| FileAttributes | 0xffff | value to indicate unset |

-------------------------------------------------------------------------------



##### FFT Matrix Packet

 I made this packet to test FFT based Reed-Solomon Codes.


The FFT matrix packet has a type value of "PAR FFT\0" (ASCII). The packet's body contains the following:

*Table: FFT Packet Body Contents*

| Length (bytes) | Type | Description |
|---------------:|:-----|:------------|
|        8       | unsigned int | Index of first input block           |
|        8       | unsigned int | Index of last input block plus 1     |
|        1       | signed int   | Max number of blocks (as power of 2) |
|      0 ~ 8     | byte[]       | Number of interleaving blocks        |


 The recovery data is computed for a range of input blocks.
All input blocks outside the range are treated as mass of zero bytes.
Even when the non-used input blocks include file data,
they are ignored at calculating recovery blocks.

 When there are 6 input blocks (0, 1, 2, 3, 4, 5),
if it computes recovery blocks for 3 input blocks (2, 3, and 4),
the first value becomes 2 and the second value becomes 5.
In the case, non-used input blocks (0, 1, and 5) become zero bytes while calculation.

 If the encoding client wants to compute recovery data for every input block,
they use the values 0 and 0.
(Because the maximum unsigned integer plus 1 rolls over to 0.)


 The third field is for max number of recovery blocks.
The value is important, because different max value makes incompatible recovery blocks.
If you have a plan to add more recovery blocks later,
you must set the max number for the possible creating blocks.

 It uses power of 2 for max number of recovery blocks.
Currently, the value is exponent of power for Low Rate encoder.
The value is like below;

|Stored value | Max number of recovery blocks
|     0       |     1
|     1       |     2
|     2       |     4
|     3       |     8
|     4       |    16
|     5       |    32
|     6       |    64
|     7       |   128
|     8       |   256
|     9       |   512
|    10       |  1024
|    11       |  2048
|    12       |  4096
|    13       |  8192
|    14       | 16384
|    15       | 32768


 It will be different format for High Rate encoder in future.
Negative value indicates High Rate encoder.
At this time, par3cmdline doesn't support High Rate encoding yet.

|Stored value | Max number of recovery blocks
|    -1       | 2 - NextPow2(number of input blocks)
|    -2       | 4 - NextPow2(number of input blocks)
|    -3       | 8 - NextPow2(number of input blocks)
...
|   -15       | 32768 - NextPow2(number of input blocks)
|   -16       | 65536 - NextPow2(number of input blocks)

NextPow2() function returns next power of two at or above given value.

 For example, when there are 100 input blocks,
NextPow2(number of input blocks) = NextPow2(100) = 128.

|Stored value | Max number of recovery blocks
|    -8       | 256 - 128 = 128
|    -9       | 512 - 128 = 384

 For example, when there are 1000 input blocks,
NextPow2(number of input blocks) = NextPow2(1000) = 1024.

|Stored value | Max number of recovery blocks
|   -11       | 2048 - 1024 = 1024
|   -12       | 4096 - 1024 = 3072
|   -13       | 8192 - 1024 = 7168


 The fourth field is for number of interleaving blocks.
Number of cohorts becomes the number of interleaving plus one.
The value is stored as little endian in 0 ~ 8 bytes.
If upper bytes are all zeros, it stores lower non zero bytes only.
When there is only one cohort (no interleaving), this field doesn't exist.

|Number of cohorts|Number of interleaving|Stored length|
|        1        |         0            |     None    |
|      2 ~ 256    |       1 ~ 255        |     1 byte  |
|    257 ~ 65536  |     256 ~ 65535      |     2 bytes |
| 65537 ~ 16777216|   65536 ~ 16777215   |     3 bytes |
|16777217 ~       |16777216 ~ 4294967295 |     4 bytes |
|4294967297 ~     |4294967296 ~          | 5 ~ 8 bytes |



##### How to interleave blocks

I explain system of interleaving here.
The interleaver splits input blocks into multiple cohorts.
Number of interleaved blocks is step between cohorts.

For example, there are 10 input blocks:
[0][1][2][3][4][5][6][7][8][9]

When it interleaves 1 block, it split blocks into 2 cohorts.
The calculation is "index modulo 2".
Because "3 mod 2 = 1", block[3] goes into cohort[1].
Because "6 mod 2 = 0", block[6] goes into cohort[0].
Then, each cohort has 5 input blocks:
cohort[0] : [0][2][4][6][8]
cohort[1] : [1][3][5][7][9]

When it interleaves 2 blocks, it split blocks into 3 cohorts.
The calculation is "index modulo 3".
Because "2 mod 3 = 2", block[2] goes into cohort[2].
Because "7 mod 3 = 1", block[7] goes into cohort[1].
Then, each cohort has 3 ~ 4 input blocks:
cohort[0] : [0][3][6][9]
cohort[1] : [1][4][7]
cohort[2] : [2][5][8]

To be simple calculation, every cohort should have same number of blocks.
When number of blocks is less than others, it gets a dummy block with all zero bytes.
Thus, each cohort contains the same number of (input and dummy) blocks:
cohort[0] : [0][3][6][9]
cohort[1] : [1][4][7][ ]
cohort[2] : [2][5][8][ ]

When it interleaves 3 blocks, it split blocks into 4 cohorts.
The calculation is "index modulo 4".
By adding dummy blocks, each cohort has 3 blocks:
cohort[0] : [0][4][8]
cohort[1] : [1][5][9]
cohort[2] : [2][6][ ]
cohort[3] : [3][7][ ]



##### How to create recovery blocks for interleaving

As I expaln above, each cohort has same number of (input and dummy) blocks.
To be simple calculation, every cohort should create same number of recovery blocks.
Therefore, number of creating recovery blocks will be multiple of number of cohorts.

For example, I create 50% (or over) redundancy for 10 input blocks.
Without interleaving, 5 recovery blocks are made from 10 input blocks:
input [0][1][2][3][4][5][6][7][8][9] -> recovery [0][1][2][3][4]

When it interleaves 1 block, there are 2 cohorts with 5 blocks each.
In each cohort, 3 recovery blocks are made from 5 input blocks.
To distinguish recovery blocks of different cohorts, they have global index.
The index of recovery block is same order as input blocks.
In total, 6 recovery blocks are made from 10 input blocks:
cohort[0] : input [0][2][4][6][8] -> recovery [0][2][4]
cohort[1] : input [1][3][5][7][9] -> recovery [1][3][5]

When it interleaves 2 blocks, there are 3 cohorts with 4 blocks each.
In each cohort, 2 recovery blocks are made from 4 (input and dummy) blocks.
In total, 6 recovery blocks are made from 10 input blocks:
cohort[0] : input [0][3][6][9] -> recovery [0][3]
cohort[1] : input [1][4][7][ ] -> recovery [1][4]
cohort[2] : input [2][5][8][ ] -> recovery [2][5]

When it interleaves 3 blocks, there are 4 cohorts with 3 blocks each.
In each cohort, 2 recovery blocks are made from 3 (input and dummy) blocks.
In total, 8 recovery blocks are made from 10 input blocks:
cohort[0] : input [0][4][8] -> recovery [0][4]
cohort[1] : input [1][5][9] -> recovery [1][5]
cohort[2] : input [2][6][ ] -> recovery [2][6]
cohort[3] : input [3][7][ ] -> recovery [3][7]

